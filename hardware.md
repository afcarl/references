Fast algorithms for convolutional neural networks
Basic computation of the optimized Winograd algorithm.
Keywords: Winograd, CNN.

Minimizing computation in convolutional neural networks
Strassen MM algorithm applied to CNNs. Good first step to understanding the idea behind using Winograd-Coppersmith.
Table 1 results seem suspect. Layer 1 convolution takes half as much as layer 1 maxpooling. This is unlikely.
Keywords: Winograd, CNN.

Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
Deep compression.
Keywords: Weight Compression.

Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks
MIT's CNN chip..
Keywords: Eyeriss, CNN, ASIC.

Caffeine: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827589
Caffeine: Towards Uniformed Representation and Acceleration for Deep Convolutional Neural Networks
Chen Zhang1,2,3∗, Zhenman Fang2 , Peipei Zhou2 , Peichen Pan3 , and Jason Cong1,2,3†

Neurocube: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551408
Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory
Duckhwan Kim∗, Jaeha Kung∗, Sek Chai†, Sudhakar Yalamanchili ∗, and Saibal Mukhopadhyay∗

Sze review:
Efficient Processing of Deep Neural Networks: A Tutorial and Survey
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel Emer
https://arxiv.org/abs/1703.09039
